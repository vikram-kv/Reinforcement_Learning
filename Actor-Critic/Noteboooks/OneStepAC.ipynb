{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P_DODRgW_ZKS"},"outputs":[],"source":["import numpy as np\n","from collections import namedtuple, deque\n","import datetime\n","import gym\n","import glob\n","import io\n","import base64\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from PIL import Image\n","import tensorflow_probability as tfp\n","import seaborn as sbn\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIB95pAAStXP","executionInfo":{"status":"ok","timestamp":1679678525246,"user_tz":-330,"elapsed":19755,"user":{"displayName":"K V Vikram cs19b021","userId":"14338776471692009956"}},"outputId":"87853ede-3943-4c3e-bc13-8ec402692632"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zt6xX0njbJvS"},"source":["## One-Step Actor-Critic Algorithm"]},{"cell_type":"markdown","metadata":{"id":"w9WWiiNxhM7x"},"source":["**Actor-Critic methods** learn both a policy $\\pi(a|s;\\theta)$ and a state-value function $v(s;w)$ simultaneously. The policy is referred to as the actor that suggests actions given a state. The estimated value function is referred to as the critic. It evaluates actions taken by the actor based on the given policy. In this exercise, both functions are approximated by feedforward neural networks. \n","\n","- The policy network is parametrized by $\\theta$ - it takes a state $s$ as input and outputs the probabilities $\\pi(a|s;\\theta)\\ \\forall\\ a$\n","- The value network is parametrized by $w$ - it takes a state $s$ as input and outputs a scalar value associated with the state, i.e., $v(s;w)$\n","- The single step TD error can be defined as follows:\n","$$\\delta_t  = R_{t+1} + \\gamma v(s_{t+1};w) - v(s_t;w)$$\n","- The loss function to be minimized at every step ($L_{tot}^{(t)}$) is a summation of two terms, as follows:\n","$$L_{tot}^{(t)} = L_{actor}^{(t)} + L_{critic}^{(t)}$$\n","where,\n","$$L_{actor}^{(t)} = -\\log\\pi(a_t|s_t; \\theta)\\delta_t$$\n","$$L_{critic}^{(t)} = \\delta_t^2$$\n","- **NOTE: Here, weights of the first two hidden layers are shared by the policy and the value network**\n","    - First two hidden layer sizes: [1024, 512]\n","    - Output size of policy network: 2 (Softmax activation)\n","    - Output size of value network: 1 (Linear activation)\n","\n","<!-- $$\\pi(a|s;\\theta) = \\phi_{\\theta}(a,s)$$ -->"]},{"cell_type":"markdown","metadata":{"id":"1eU3_IeYwN3H"},"source":["### Actor-Critic Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXFHQcJjVKYu"},"outputs":[],"source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1800\n","AVG_NUM = 50\n","SOLVE_CUTOFF = 195\n","\n","class ActorCriticModel(tf.keras.Model):\n","    \"\"\"\n","    Defining policy and value networkss\n","    \"\"\"\n","    def __init__(self, action_size):\n","        super(ActorCriticModel, self).__init__()\n","\n","        self.fcls = []\n","        for i in range(NUM_HIDDEN):\n","            fc = tf.keras.layers.Dense(HIDDEN_SIZES[i], activation='relu')\n","            self.fcls.append(fc)\n","\n","        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax') #Output Layer for policy\n","        self.v_out = tf.keras.layers.Dense(1) #Output Layer for state-value\n","\n","    def call(self, state):\n","        \"\"\"\n","        Computes policy distribution and state-value for a given state\n","        \"\"\"\n","        out = state\n","        for l in self.fcls:\n","            out = l(out)\n","\n","        pi = self.pi_out(out)\n","        v = self.v_out(out)\n","\n","        return pi, v"]},{"cell_type":"markdown","metadata":{"id":"40uV1hrewVnA"},"source":["### Agent Class\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RNpp9WMfkTE"},"outputs":[],"source":["class Agent:\n","    \"\"\"\n","    Agent class\n","    \"\"\"\n","    def __init__(self, action_size, gamma=0.99):\n","        self.gamma = gamma\n","        self.ac_model = ActorCriticModel(action_size=action_size)\n","        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=LR))\n","    \n","    def sample_action(self, state):\n","        \"\"\"\n","        Given a state, compute the policy distribution over all actions and sample one action\n","        \"\"\"\n","        pi,_ = self.ac_model(state)\n","\n","        action_probabilities = tfp.distributions.Categorical(probs=pi)\n","        sample = action_probabilities.sample()\n","        return int(sample.numpy()[0])\n","\n","    def actor_loss(self, action, pi, delta):\n","        \"\"\"\n","        Compute Actor Loss\n","        \"\"\"\n","        return -tf.math.log(pi[0,action]) * delta\n","\n","    def critic_loss(self,delta):\n","        \"\"\"\n","        Critic loss aims to minimize TD error\n","        \"\"\"\n","        return delta**2\n","\n","    @tf.function\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        For a given transition (s,a,s',r) update the paramters by computing the\n","        gradient of the total loss\n","        \"\"\"\n","        with tf.GradientTape(persistent=True) as tape:\n","            pi, V_s = self.ac_model(state)\n","            _, V_s_next = self.ac_model(next_state)\n","\n","            V_s = tf.squeeze(V_s)\n","            V_s_next = tf.squeeze(V_s_next)\n","            \n","            #### TO DO: Write the equation for delta (TD error)\n","            ## Write code below\n","            delta = reward + (self.gamma * V_s_next) - V_s\n","            loss_a = self.actor_loss(action, pi, delta)\n","            loss_c = self.critic_loss(delta)\n","            loss_total = loss_a + loss_c\n","        \n","        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n","        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"]},{"cell_type":"markdown","metadata":{"id":"bUJwznIzwBIX"},"source":["\n","### Code to train the agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0SB0o_OfyGN"},"outputs":[],"source":["# do 1 run of the AC-agent in an environment for num_episodes\n","def run_ac_net(env, tf_seed=42):\n","\n","    agent = Agent(action_size=env.action_space.n)\n","    tf.keras.utils.set_random_seed(tf_seed)\n","    tf.compat.v1.reset_default_graph()\n","\n","    reward_list = []\n","    step_list = []\n","    begin_time = datetime.datetime.now()\n","    ep_count = NUM_EPISODES\n","\n","    for ep in range(1, NUM_EPISODES + 1):\n","        state = env.reset().reshape(1,-1)\n","        done = False\n","        ep_rew = 0\n","        steps = 0\n","        while not done:\n","            action = agent.sample_action(state) ##Sample Action\n","            next_state, reward, done, info = env.step(action) ##Take action\n","            next_state = next_state.reshape(1,-1)\n","            ep_rew += reward  ##Updating episode reward\n","            agent.learn(state, action, reward, next_state, done) ##Update Parameters\n","            state = next_state ##Updating State\n","            steps += 1\n","        reward_list.append(ep_rew)\n","        step_list.append(steps)\n","    \n","        if ep % 10 == 0:\n","            avg_rew = np.mean(reward_list[-10:])\n","            print('Episode ', ep, 'Reward %f' % ep_rew, 'Average Reward %f' % avg_rew)\n","\n","        if ep % 100:\n","            avg_100 =  np.mean(reward_list[-100:])\n","            if avg_100 > SOLVE_CUTOFF:\n","                ep_count = min(ep_count, ep) # keep track of number of episodes to solve environment\n","\n","    time_taken = datetime.datetime.now() - begin_time\n","    print(time_taken)\n","    return reward_list, step_list, ep_count"]},{"cell_type":"code","source":["def rolling_average(reward_list, avg_num=AVG_NUM):\n","    average_reward_list = np.zeros_like(reward_list)\n","    for i in range(0, len(reward_list)):\n","        if i < avg_num:\n","            average_reward_list[i] = np.mean(reward_list[:(i+1)])\n","        else:\n","            average_reward_list[i] = np.mean(reward_list[(i-avg_num+1):(i+1)])\n","    return average_reward_list"],"metadata":{"id":"jrxpw5qRNP5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def do_multiple_runs(env_name, num_runs):\n","    reward_lists, step_lists, comp_list = [], [], []\n","    env = gym.make(env_name)\n","    env.seed(42)\n","\n","    for r in range(num_runs):\n","        r, s, c = run_ac_net(env)\n","        reward_lists.append(r)\n","        step_lists.append(list(map(int,s)))\n","        comp_list.append(c)\n","    \n","    mean_rewards = np.mean(reward_lists, axis=0)\n","    var_rewards = np.sqrt(np.var(reward_lists, axis=0))\n","\n","    return mean_rewards, var_rewards, comp_list, reward_lists, step_lists"],"metadata":{"id":"1fAqwcBnpOGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plotting Functions and Other Helpers"],"metadata":{"id":"WHcpXW3NNKPJ"}},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/Colab Notebooks/CS6700/Assignments/Assignment2/AC1/'\n","\n","def gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists):\n","    \n","    m_fig, m_ax = plt.subplots(figsize=(10,10))\n","    v_fig, v_ax = plt.subplots(figsize=(10,10))\n","    m_ax.set_title(f'Mean Reward over 10 runs')\n","    m_ax.set_xlabel('Episodes')\n","    m_ax.set_ylabel('Mean Reward')\n","    v_ax.set_title(f'Variance of Reward over 10 runs')\n","    v_ax.set_xlabel('Episodes')\n","    v_ax.set_ylabel('Variance')\n","\n","    sbn.lineplot(data=mean_rewards, ax=m_ax)\n","    sbn.lineplot(data=var_rewards, ax=v_ax)\n","      \n","    m_fig.savefig(PATH+'/'+fname+'/mean_reward.png')\n","    v_fig.savefig(PATH+'/'+fname+'/var_reward.png')\n","\n","    sc_fig, sc_ax = plt.subplots(figsize=(10,10))\n","    st_fig, st_ax = plt.subplots(figsize=(10,10))\n","    sc_ax.set_title(f'Running average of previous {AVG_NUM} rewards')\n","    sc_ax.set_xlabel('Episodes')\n","    sc_ax.set_ylabel('Reward')\n","    st_ax.set_title(f'Number of steps curve')\n","    st_ax.set_xlabel('Episodes')\n","    st_ax.set_ylabel('Steps')\n","    \n","    run_count = len(reward_lists)\n","    labels = [f'run {i+1}' for i in range(3)]\n","    for idx in range(min(run_count, 3)):\n","\n","        score_list = rolling_average(reward_lists[idx])\n","        step_list = step_lists[idx]\n","        sbn.lineplot(data=score_list, label=labels[idx], ax=sc_ax)\n","        sbn.lineplot(data=step_list, label=labels[idx], ax=st_ax)\n","\n","    sc_fig.savefig(PATH+'/'+fname+'/sc_fig.png')\n","    st_fig.savefig(PATH+'/'+fname+'/st_fig.png')\n","\n","    with open(PATH+'/'+fname+'/'+'config.txt', 'w') as f:\n","        for s in data:\n","            f.write(s + '\\n')"],"metadata":{"id":"BkxrYOynNgrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Experiments"],"metadata":{"id":"MVYsX2JURooL"}},{"cell_type":"code","source":["# STEP = 0 does not mean anything. We are running 1 step AC."],"metadata":{"id":"8f-Ff4PnJFwl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### CartPole"],"metadata":{"id":"sii_BkC_4CE-"}},{"cell_type":"code","source":["NUM_HIDDEN = 1\n","HIDDEN_SIZES = [1024]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"Juo_sP9G3siO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 0.5e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"Xl1ngOOR3tjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"3Znfc3C73_hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 2e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"ME9W_Rl-3_sZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"NYjg1y-P3_0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 0.5e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"qzlVlSh24AOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"pZRAw3BI4xkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 2e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"8lUQWJmI45Vt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 3\n","HIDDEN_SIZES = [512, 1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'CartPole-v1'\n","SOLVE_CUTOFF = 195.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"W3E3BnCx5JMn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Acrobot"],"metadata":{"id":"AIvOSuZB5XQ7"}},{"cell_type":"code","source":["NUM_HIDDEN = 1\n","HIDDEN_SIZES = [1024]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"nEFmrFEO5XQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 0.5e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"U89gIlFj5XQ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"4U6r5lDW5XQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [512, 512]\n","LR = 2e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"SVaCPDZv5XQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"c2dwBIEA5XQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 0.5e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"FFPD_2Gy5XRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"5CBW3vNR5XRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 1024]\n","LR = 2e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"3yyBxcmk5XRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NUM_HIDDEN = 3\n","HIDDEN_SIZES = [512, 1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 1000\n","AVG_NUM = 50\n","ENV_NAME = 'Acrobot-v1'\n","SOLVE_CUTOFF = -100.0\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"TitW_1F-5XRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MountainCar"],"metadata":{"id":"VkYBdQqJ6KqQ"}},{"cell_type":"code","source":["NUM_HIDDEN = 2\n","HIDDEN_SIZES = [1024, 512]\n","LR = 1e-4\n","NUM_EPISODES = 600\n","AVG_NUM = 50\n","ENV_NAME = 'MountainCar-v0'\n","SOLVE_CUTOFF = -125\n","STEP = 0\n","\n","ENV = gym.make(ENV_NAME)\n","mean_rewards, var_rewards, comp_list, reward_lists, step_lists = do_multiple_runs(env_name=ENV_NAME, num_runs=10)\n","\n","data = [f'Number of Hidden Layers - {NUM_HIDDEN}', f'Hidden Sizes - {HIDDEN_SIZES}', f'Learning Rate - {LR}', \n","        f'Run number of episodes to solve - {comp_list}', f'Mean number of episodes to solve- {np.mean(comp_list)}', \n","        f'Number of episodes - {NUM_EPISODES}', f'Environment Name - {ENV_NAME}', f'NSTEP = {STEP}']\n","\n","fname = f'hl={NUM_HIDDEN}_hs={HIDDEN_SIZES}_lr={LR}_env={ENV_NAME}_numep={NUM_EPISODES}_nstep={STEP}'\n","os.mkdir(PATH+'/'+fname)\n","gen_plots_and_save_data(fname, data, mean_rewards, var_rewards, reward_lists, step_lists)"],"metadata":{"id":"SvXqkGyV6QKF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1CL_N2PdFArdiRxkHV7jzZx0KM3UFObG0","timestamp":1679575543148}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}